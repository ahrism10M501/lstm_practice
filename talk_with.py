# generated by ChatGPT

import json
import pickle

import torch
import torch.nn as nn
from LSTM import LSTM
from soynlp.tokenizer import MaxScoreTokenizer
# DataCollector 초기화
data_path = '/dadad'

with open('word_score.pkl', 'rb') as score_f:
    word_score = pickle.load(score_f)
tokenizer = MaxScoreTokenizer(scores=word_score)  # 토크나이저 초기화

with open("vocab.json", "r", encoding="utf-8") as vocab_file:
    vocab = json.load(vocab_file)

with open("hyper_param.json", "r", encoding="utf-8") as hpf:
    hyper_param = json.load(hpf)

# 이후 generate 함수에 이 tokenizer와 vocab을 전달

# 1. 모델 초기화 및 가중치 로드
def load_model(model_path, vocab_size, embedding_dim, hidden_size, layer, output_dim, bias=True, device='cpu'):
    model = LSTM(
        vocab_size=vocab_size,
        embedding_dim=embedding_dim,
        hidden_size=hidden_size,
        layer=layer,
        output=output_dim,
        bias=bias
    )
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()
    return model

# 2. 토큰화 + 인덱스화 (간단 예시 - 실제로는 tokenizer와 vocab을 그대로 사용)
def encode_input(text, tokenizer, vocab):
    tokens = tokenizer.tokenize(text)
    return [vocab.get(t, vocab['<UNK>']) for t in tokens]

# 3. 생성 함수 (빔서치나 샘플링 없이 greedy)
def generate(model, tokenizer, vocab, start_text, max_len=20, device='cpu'):
    model.eval()
    input_tokens = encode_input(start_text, tokenizer, vocab)
    input_tensor = torch.tensor([input_tokens], dtype=torch.long).to(device)  # [1, seq_len]

    generated = input_tokens.copy()

    with torch.no_grad():
        for _ in range(max_len):
            outputs = model(input_tensor)  # [1, seq_len, vocab_size]
            logits = outputs[0, -1]        # 마지막 토큰 예측 [vocab_size]
            _, predicted = torch.max(logits, dim=-1)  # 예측 토큰 인덱스
            
            next_token = predicted.item()
            generated.append(next_token)
            
            input_tensor = torch.tensor([generated], dtype=torch.long).to(device)

            # 종료 토큰(예: <EOS>) 조건 있으면 여기서 체크 후 break 가능

    # 인덱스 → 토큰 역변환
    inv_vocab = {v:k for k, v in vocab.items()}
    gen_tokens = [inv_vocab.get(idx, '<UNK>') for idx in generated]

    return ''.join(gen_tokens)

if __name__ == "__main__":
    import sys
    device = 'cuda' if torch.cuda.is_available() else 'cpu'


    # ## model = LSTM(
    #             vocab_size=len(vocab), 
    #             embedding_dim=hyper_param['embedding_size'],
    #             hidden_size=hyper_param['hidden_size'],
    #             layer=hyper_param['layer'],
    #             output=len(vocab),
    #             bias=hyper_param['bias']
    #         )
    # 하이퍼파라미터는 학습 스크립트와 동일하게 맞출 것
    model = load_model(
        model_path='best_model.pt',
        vocab_size=len(vocab),
        embedding_dim=hyper_param['embedding_size'],  # 학습한 값과 동일하게
        hidden_size=hyper_param['hidden_size'],
        layer=hyper_param['layer'],
        output_dim=len(vocab),
        bias=hyper_param['bias'],
        device=device
    )

    prompt = input("Input your prompt: ")
    generated_text = generate(model, tokenizer, vocab, prompt, max_len=30, device=device)
    print("Generated:", generated_text)
