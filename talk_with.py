# generated by ChatGPT

import torch
import torch.nn as nn
from LSTM import LSTM
from data_collector import DataCollector

# DataCollector 초기화
data_path = 'path/to/data'
collector = DataCollector(data_path, key='sentence', mode='valid')
collector.tokenizerTrain(path='word_score.pkl')  # 토크나이저 초기화
vocab = collector.makeVocab(path='vocab.json')   # vocab 불러오기 또는 생성

tokenizer = DataCollector.tokenizer  # 클래스 변수에서 토크나이저 접근

# 이후 generate 함수에 이 tokenizer와 vocab을 전달

# 1. 모델 초기화 및 가중치 로드
def load_model(model_path, vocab_size, embedding_dim, hidden_size, layer, output_dim, bias=True, device='cpu'):
    model = LSTM(
        vocab_size=vocab_size,
        embedding_dim=embedding_dim,
        hidden_size=hidden_size,
        layer=layer,
        output=output_dim,
        bias=bias
    )
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()
    return model

# 2. 토큰화 + 인덱스화 (간단 예시 - 실제로는 tokenizer와 vocab을 그대로 사용)
def encode_input(text, tokenizer, vocab):
    tokens = tokenizer.tokenize(text)
    return [vocab.get(t, vocab['<UNK>']) for t in tokens]

# 3. 생성 함수 (빔서치나 샘플링 없이 greedy)
def generate(model, tokenizer, vocab, start_text, max_len=20, device='cpu'):
    model.eval()
    input_tokens = encode_input(start_text, tokenizer, vocab)
    input_tensor = torch.tensor([input_tokens], dtype=torch.long).to(device)  # [1, seq_len]

    generated = input_tokens.copy()

    with torch.no_grad():
        for _ in range(max_len):
            outputs = model(input_tensor)  # [1, seq_len, vocab_size]
            logits = outputs[0, -1]        # 마지막 토큰 예측 [vocab_size]
            _, predicted = torch.max(logits, dim=-1)  # 예측 토큰 인덱스
            
            next_token = predicted.item()
            generated.append(next_token)
            
            input_tensor = torch.tensor([generated], dtype=torch.long).to(device)

            # 종료 토큰(예: <EOS>) 조건 있으면 여기서 체크 후 break 가능

    # 인덱스 → 토큰 역변환
    inv_vocab = {v:k for k, v in vocab.items()}
    gen_tokens = [inv_vocab.get(idx, '<UNK>') for idx in generated]

    return ''.join(gen_tokens)

if __name__ == "__main__":
    import sys
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # 하이퍼파라미터는 학습 스크립트와 동일하게 맞출 것
    model = load_model(
        model_path='best_model.pt',
        vocab_size=len(vocab),
        embedding_dim=128,  # 학습한 값과 동일하게
        hidden_size=256,
        layer=3,
        output_dim=64,
        bias=True,
        device=device
    )

    prompt = input("Input your prompt: ")
    generated_text = generate(model, tokenizer, vocab, prompt, max_len=30, device=device)
    print("Generated:", generated_text)
