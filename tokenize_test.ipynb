{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50bc8392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.noun import LRNounExtractor_v2\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cc4816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.utils import DoublespaceLineCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a92629d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = DoublespaceLineCorpus('token_example.txt', iter_sent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be155d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 58 from 5 sents. mem=5.284 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=63, mem=5.284 Gb\n",
      "[Noun Extractor] batch prediction was completed for 23 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 3 -> 3\n",
      "[Noun Extractor] postprocessing ignore_features : 3 -> 3\n",
      "[Noun Extractor] postprocessing ignore_NJ : 3 -> 3\n",
      "[Noun Extractor] 3 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=5.284 Gb                    \n",
      "[Noun Extractor] 17.46 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "noun_extractor = LRNounExtractor_v2()\n",
    "noun_extractor.train(sent)\n",
    "nouns = noun_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd4d14ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 5.284 Gb5.284 Gb\n",
      "all cohesion probabilities was computed. # words = 2\n",
      "all branching entropies was computed # words = 15\n",
      "all accessor variety was computed # words = 15\n"
     ]
    }
   ],
   "source": [
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(sent)\n",
    "word_score_table = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c60537d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': Scores(cohesion_forward=0, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=0, left_accessor_variety=0, right_accessor_variety=0, leftside_frequency=0, rightside_frequency=5),\n",
       " '고': Scores(cohesion_forward=0, cohesion_backward=0, left_branching_entropy=-0.0, right_branching_entropy=0.5297061990576545, left_accessor_variety=1, right_accessor_variety=2, leftside_frequency=7, rightside_frequency=0),\n",
       " '고양': Scores(cohesion_forward=np.float64(1.0), cohesion_backward=0, left_branching_entropy=-0.0, right_branching_entropy=-0.0, left_accessor_variety=1, right_accessor_variety=1, leftside_frequency=7, rightside_frequency=0),\n",
       " '고양이': Scores(cohesion_forward=np.float64(1.0), cohesion_backward=0, left_branching_entropy=-0.0, right_branching_entropy=0, left_accessor_variety=1, right_accessor_variety=0, leftside_frequency=7, rightside_frequency=0)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcf80f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LTokenizer(scores=word_score_table)\n",
    "tokens = tokenizer.tokenize('꽁꽁 얼어붙은 한강 위로 고양이 먐먐')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07d1d947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['꽁꽁', '얼어붙은', '한강', '위로', '고양', '이', '먐먐']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc53c67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'고양이': NounScore(frequency=4, score=1.0),\n",
       " '발전': NounScore(frequency=2, score=1.0),\n",
       " '내용': NounScore(frequency=2, score=1.0)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58eaf5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': Scores(cohesion_forward=0, cohesion_backward=0, left_branching_entropy=0, right_branching_entropy=0, left_accessor_variety=0, right_accessor_variety=0, leftside_frequency=0, rightside_frequency=5),\n",
       " '고': Scores(cohesion_forward=0, cohesion_backward=0, left_branching_entropy=-0.0, right_branching_entropy=0.5297061990576545, left_accessor_variety=1, right_accessor_variety=2, leftside_frequency=7, rightside_frequency=0),\n",
       " '고양': Scores(cohesion_forward=np.float64(1.0), cohesion_backward=0, left_branching_entropy=-0.0, right_branching_entropy=-0.0, left_accessor_variety=1, right_accessor_variety=1, leftside_frequency=7, rightside_frequency=0),\n",
       " '고양이': Scores(cohesion_forward=np.float64(1.0), cohesion_backward=0, left_branching_entropy=-0.0, right_branching_entropy=0, left_accessor_variety=1, right_accessor_variety=0, leftside_frequency=7, rightside_frequency=0)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c9fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_collector import DataCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fac0e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data Collecting : 100%|██████████| 24/24 [00:08<00:00,  2.96it/s]\n"
     ]
    }
   ],
   "source": [
    "collector = DataCollector('./data', 'train', 'sentence')\n",
    "collector.tokenizerTrain(path='word_score.pkl', save_path='word_score.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ea0d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1229306"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(collector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a58f9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [sent['sentence'] for sent in collector.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79999770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 2.939 Gbory 2.873 Gb\n",
      "all cohesion probabilities was computed. # words = 28763\n",
      "all branching entropies was computed # words = 78374\n",
      "all accessor variety was computed # words = 78374\n"
     ]
    }
   ],
   "source": [
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "\n",
    "with open('corpus.txt', 'w', encoding='utf-8') as file:\n",
    "        for se in sent[:10000]:\n",
    "            se = collector._clean_text(se)\n",
    "            file.write(se + '\\n\\n')\n",
    "\n",
    "ses = DoublespaceLineCorpus('corpus.txt', iter_sent=True)\n",
    "Word_Extractor = WordExtractor()\n",
    "Word_Extractor.train(ses)\n",
    "word_score_table:dict = Word_Extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfd20c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "scoredict = {word:score.cohesion_forward for word, score in word_score_table.items()}\n",
    "tokener = MaxScoreTokenizer(scores=scoredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c64c9533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['방가',\n",
       " '안녕',\n",
       " '하세요',\n",
       " '이',\n",
       " '문장에',\n",
       " '대해서',\n",
       " '토큰화',\n",
       " '진행',\n",
       " '하',\n",
       " '시',\n",
       " '실',\n",
       " '뭉마',\n",
       " 'ej',\n",
       " 'zmsk',\n",
       " 'flqhoasslk',\n",
       " 'nl',\n",
       " '더',\n",
       " '긴문장도',\n",
       " '되나요']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokener(collector._clean_text('방가 안녕하세요 이 문장에 대해서 토큰화 진행 하 시 ㄹ실 뭉마ㅓ ej zmsk flqhoasslknl  더 긴문장도 되나요'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "867a7d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['아사히신문은',\n",
       " '10월',\n",
       " '1',\n",
       " '8일',\n",
       " '자',\n",
       " '온라인',\n",
       " '보도를',\n",
       " '통해',\n",
       " '일본',\n",
       " '이',\n",
       " '한국',\n",
       " '에',\n",
       " '대한',\n",
       " '수출',\n",
       " '규제를',\n",
       " '시',\n",
       " '행하',\n",
       " '기',\n",
       " '전',\n",
       " '아베신조',\n",
       " '수상과',\n",
       " '내각',\n",
       " '대신들이',\n",
       " '관저',\n",
       " '집무실에서',\n",
       " '논의',\n",
       " '를',\n",
       " '하는',\n",
       " '과정에서',\n",
       " '경제',\n",
       " '산업',\n",
       " '성의',\n",
       " '신중론을',\n",
       " '제압',\n",
       " '하고',\n",
       " '수출',\n",
       " '규제를',\n",
       " '감행',\n",
       " '했다',\n",
       " '고',\n",
       " '전',\n",
       " '했다.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collector[3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
